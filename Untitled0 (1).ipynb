{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRR1XyXpkKVl",
        "outputId": "c2546641-a12c-454a-eccf-934127a189e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/assesment/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oomKKwlwI6_",
        "outputId": "a02da47c-a35f-401f-f527-453e29a041db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/assesment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAzohRXgwg2O",
        "outputId": "1f3e22d5-e2af-48b2-ad75-14d04efd39bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Untitled0.ipynb  'WHR-2024-5CS037 (1).csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem - 1: Getting Started with Data Exploration - Some Warm up\n",
        "Exercises:"
      ],
      "metadata": {
        "id": "mmeBPN2twri_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define the file path (Modify if necessary)\n",
        "file_path =  'WHR-2024-5CS037 (1).csv'  # Make sure the file path is correct\n",
        "\n",
        "# Function to attempt loading the dataset with different encodings\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            # Attempt to load the CSV file with a specific encoding\n",
        "            print(f\"Attempting to load the file with encoding: {encoding}\")\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed to load the file with encoding: {encoding}\")\n",
        "        except pd.errors.ParserError:\n",
        "            print(f\"Parsing error with encoding: {encoding}. The file may be corrupted or formatted incorrectly.\")\n",
        "            break\n",
        "    return None  # Return None if all attempts fail\n",
        "\n",
        "# Step 2: Try loading the CSV file with different encodings\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# Step 3: If the dataset was loaded successfully, perform further operations\n",
        "if df is not None:\n",
        "    try:\n",
        "        # Display the first 10 rows of the dataset\n",
        "        print(\"\\nFirst 10 rows of the dataset:\")\n",
        "        print(df.head(10))\n",
        "\n",
        "        # Identify the number of rows and columns in the dataset\n",
        "        rows, columns = df.shape\n",
        "        print(f\"\\nNumber of rows: {rows}\")\n",
        "        print(f\"Number of columns: {columns}\")\n",
        "\n",
        "        # List all the columns and their data types\n",
        "        print(\"\\nData types of all columns:\")\n",
        "        print(df.dtypes)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing the dataset: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sMjvBGtPxD5-",
        "outputId": "b2de94db-b15d-490d-fff1-3858aa4d6c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 rows of the dataset:\n",
            "  Country name  score  Log GDP per capita  Social support  \\\n",
            "0      Finland  7.741               1.844           1.572   \n",
            "1      Denmark  7.583               1.908           1.520   \n",
            "2      Iceland  7.525               1.881           1.617   \n",
            "3       Sweden  7.344               1.878           1.501   \n",
            "4       Israel  7.341               1.803           1.513   \n",
            "5  Netherlands  7.319               1.901           1.462   \n",
            "6       Norway  7.302               1.952           1.517   \n",
            "7   Luxembourg  7.122               2.141           1.355   \n",
            "8  Switzerland  7.060               1.970           1.425   \n",
            "9    Australia  7.057               1.854           1.461   \n",
            "\n",
            "   Healthy life expectancy  Freedom to make life choices  Generosity  \\\n",
            "0                    0.695                         0.859       0.142   \n",
            "1                    0.699                         0.823       0.204   \n",
            "2                    0.718                         0.819       0.258   \n",
            "3                    0.724                         0.838       0.221   \n",
            "4                    0.740                         0.641       0.153   \n",
            "5                    0.706                         0.725       0.247   \n",
            "6                    0.704                         0.835       0.224   \n",
            "7                    0.708                         0.801       0.146   \n",
            "8                    0.747                         0.759       0.173   \n",
            "9                    0.692                         0.756       0.225   \n",
            "\n",
            "   Perceptions of corruption  Dystopia + residual  \n",
            "0                      0.546                2.082  \n",
            "1                      0.548                1.881  \n",
            "2                      0.182                2.050  \n",
            "3                      0.524                1.658  \n",
            "4                      0.193                2.298  \n",
            "5                      0.372                1.906  \n",
            "6                      0.484                1.586  \n",
            "7                      0.432                1.540  \n",
            "8                      0.498                1.488  \n",
            "9                      0.323                1.745  \n",
            "\n",
            "Number of rows: 143\n",
            "Number of columns: 9\n",
            "\n",
            "Data types of all columns:\n",
            "Country name                     object\n",
            "score                           float64\n",
            "Log GDP per capita              float64\n",
            "Social support                  float64\n",
            "Healthy life expectancy         float64\n",
            "Freedom to make life choices    float64\n",
            "Generosity                      float64\n",
            "Perceptions of corruption       float64\n",
            "Dystopia + residual             float64\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Score'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Score'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c387bcccb5b5>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Basic Statistics for 'Score'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmean_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mmedian_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mstd_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Score'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define the file path (Modify if necessary)\n",
        "file_path = 'WHR-2024-5CS037 (1).csv'  # Make sure the file path is correct\n",
        "\n",
        "# Function to attempt loading the dataset with different encodings\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            # Attempt to load the CSV file with a specific encoding\n",
        "            print(f\"Attempting to load the file with encoding: {encoding}\")\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed to load the file with encoding: {encoding}\")\n",
        "        except pd.errors.ParserError:\n",
        "            print(f\"Parsing error with encoding: {encoding}. The file may be corrupted or formatted incorrectly.\")\n",
        "            break\n",
        "    return None  # Return None if all attempts fail\n",
        "\n",
        "# Step 2: Try loading the CSV file with different encodings\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# Step 3: If the dataset was loaded successfully, perform further operations\n",
        "if df is not None:\n",
        "    try:\n",
        "        # Step 4: Calculate the mean, median, and standard deviation for the \"Score\" column\n",
        "        if 'Score' in df.columns:\n",
        "            mean_score = df['Score'].mean()\n",
        "            median_score = df['Score'].median()\n",
        "            std_score = df['Score'].std()\n",
        "\n",
        "            print(f\"\\nMean of Score: {mean_score}\")\n",
        "            print(f\"Median of Score: {median_score}\")\n",
        "            print(f\"Standard Deviation of Score: {std_score}\")\n",
        "\n",
        "        else:\n",
        "            print(\"Error: 'Score' column not found in the dataset.\")\n",
        "\n",
        "        # Step 5: Identify the country with the highest and lowest happiness scores\n",
        "        if 'Country' in df.columns and 'Score' in df.columns:\n",
        "            # Country with the highest score\n",
        "            highest_score_country = df.loc[df['Score'].idxmax(), 'Country']\n",
        "            highest_score = df['Score'].max()\n",
        "\n",
        "            # Country with the lowest score\n",
        "            lowest_score_country = df.loc[df['Score'].idxmin(), 'Country']\n",
        "            lowest_score = df['Score'].min()\n",
        "\n",
        "            print(f\"\\nCountry with the highest happiness score: {highest_score_country} ({highest_score})\")\n",
        "            print(f\"Country with the lowest happiness score: {lowest_score_country} ({lowest_score})\")\n",
        "        else:\n",
        "            print(\"Error: 'Country' or 'Score' column not found in the dataset.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing the dataset: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KZH4n0cy5M7",
        "outputId": "9dd528f6-445e-4c2b-f553-4045c3c49a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 143\n",
            "Number of columns: 9\n",
            "Columns in the dataset: Index(['Country name', 'score', 'Log GDP per capita', 'Social support',\n",
            "       'Healthy life expectancy', 'Freedom to make life choices', 'Generosity',\n",
            "       'Perceptions of corruption', 'Dystopia + residual'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define the file path (Modify if necessary)\n",
        "file_path =  'WHR-2024-5CS037 (1).csv'  # Make sure the file path is correct\n",
        "\n",
        "# Function to attempt loading the dataset with different encodings\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            # Attempt to load the CSV file with a specific encoding\n",
        "            print(f\"Attempting to load the file with encoding: {encoding}\")\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed to load the file with encoding: {encoding}\")\n",
        "        except pd.errors.ParserError:\n",
        "            print(f\"Parsing error with encoding: {encoding}. The file may be corrupted or formatted incorrectly.\")\n",
        "            break\n",
        "    return None  # Return None if all attempts fail\n",
        "\n",
        "# Step 2: Try loading the CSV file with different encodings\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# Step 3: If the dataset was loaded successfully, perform further operations\n",
        "if df is not None:\n",
        "    try:\n",
        "        # Step 4: Check for missing values in the dataset\n",
        "        missing_values = df.isnull().sum()\n",
        "\n",
        "        # Display the total count of missing values for each column\n",
        "        print(\"\\nMissing values count for each column:\")\n",
        "        print(missing_values)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing the dataset: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "id": "ldBIIovpKS2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define the file path (Modify if necessary)\n",
        "file_path =  'WHR-2024-5CS037 (1).csv'  # Make sure the file path is correct\n",
        "\n",
        "# Function to attempt loading the dataset with different encodings\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            # Attempt to load the CSV file with a specific encoding\n",
        "            print(f\"Attempting to load the file with encoding: {encoding}\")\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed to load the file with encoding: {encoding}\")\n",
        "        except pd.errors.ParserError:\n",
        "            print(f\"Parsing error with encoding: {encoding}. The file may be corrupted or formatted incorrectly.\")\n",
        "            break\n",
        "    return None  # Return None if all attempts fail\n",
        "\n",
        "# Step 2: Try loading the CSV file with different encodings\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# Step 3: If the dataset was loaded successfully, perform further operations\n",
        "if df is not None:\n",
        "    try:\n",
        "        # Step 4: Filter the dataset to show only the countries with a Score greater than 7.5\n",
        "        filtered_df = df[df['Score'] > 7.5]\n",
        "\n",
        "        # Step 5: Sort the filtered dataset by GDP per Capita in descending order\n",
        "        sorted_df = filtered_df.sort_values(by='GDP per Capita', ascending=False)\n",
        "\n",
        "        # Step 6: Display the top 10 rows of the sorted dataset\n",
        "        print(\"\\nTop 10 countries with Score > 7.5, sorted by GDP per Capita:\")\n",
        "        print(sorted_df.head(10))\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: The column '{e.args[0]}' was not found in the dataset.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing the dataset: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "id": "8cAxUE-GKkJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define the file path (Modify if necessary)\n",
        "file_path = 'WHR-2024-5CS037 (1).csv'  # Make sure the file path is correct\n",
        "\n",
        "# Function to attempt loading the dataset with different encodings\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            # Attempt to load the CSV file with a specific encoding\n",
        "            print(f\"Attempting to load the file with encoding: {encoding}\")\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed to load the file with encoding: {encoding}\")\n",
        "        except pd.errors.ParserError:\n",
        "            print(f\"Parsing error with encoding: {encoding}. The file may be corrupted or formatted incorrectly.\")\n",
        "            break\n",
        "    return None  # Return None if all attempts fail\n",
        "\n",
        "# Step 2: Try loading the CSV file with different encodings\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# Step 3: If the dataset was loaded successfully, perform further operations\n",
        "if df is not None:\n",
        "    try:\n",
        "        # Step 4: Create a new column \"Happiness Category\" based on \"Score\"\n",
        "        def categorize_happiness(score):\n",
        "            if score < 4:\n",
        "                return 'Low'\n",
        "            elif 4 <= score <= 6:\n",
        "                return 'Medium'encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "id": "4tM8PIhaK8jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Define the file path (Modify if necessary)\n",
        "file_path =  'WHR-2024-5CS037 (1).csv'  # Make sure the file path is correct\n",
        "\n",
        "# Function to attempt loading the dataset with different encodings\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            # Attempt to load the CSV file with a specific encoding\n",
        "            print(f\"Attempting to load the file with encoding: {encoding}\")\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed to load the file with encoding: {encoding}\")\n",
        "        except pd.errors.ParserError:\n",
        "            print(f\"Parsing error with encoding: {encoding}. The file may be corrupted or formatted incorrectly.\")\n",
        "            break\n",
        "    return None  # Return None if all attempts fail\n",
        "\n",
        "# Step 2: Try loading the CSV file with different encodings\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# Step 3: If the dataset was loaded successfully, perform further operations\n",
        "if df is not None:\n",
        "    try:\n",
        "        # Step 4: Sort the dataset by Score in descending order and select the top 10 rows\n",
        "        top_10_happiest = df.sort_values(by='Score', ascending=False).head(10)\n",
        "\n",
        "        # Step 5: Create the bar plot for the top 10 happiest countries\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='Score', y='Country', data=top_10_happiest, palette='viridis')\n",
        "\n",
        "        # Adding labels and title\n",
        "        plt.xlabel('Happiness Score')\n",
        "        plt.ylabel('Country')\n",
        "        plt.title('Top 10 Happiest Countries by Happiness Score')\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: The column '{e.args[0]}' was not found in the dataset.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing the dataset: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "id": "ib4mNORSLS4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Define the file path (Modify if necessary)\n",
        "file_path =  'WHR-2024-5CS037 (1).csv' # Ensure the file path is correct\n",
        "\n",
        "# Function to attempt loading the dataset with different encodings\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            # Attempt to load the CSV file with a specific encoding\n",
        "            print(f\"Attempting to load the file with encoding: {encoding}\")\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed to load the file with encoding: {encoding}\")\n",
        "        except pd.errors.ParserError:\n",
        "            print(f\"Parsing error with encoding: {encoding}. The file may be corrupted or formatted incorrectly.\")\n",
        "            break\n",
        "    return None  # Return None if all attempts fail\n",
        "\n",
        "# Step 2: Try loading the CSV file with different encodings\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# Step 3: If the dataset was loaded successfully, perform further operations\n",
        "if df is not None:\n",
        "    try:\n",
        "        # Plot 1: Line Plot of the Top 10 Unhappiest Countries by Score\n",
        "        unhappiest_countries = df.sort_values(by='Score').head(10)  # Get the lowest 10 scores\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.lineplot(x='Country', y='Score', data=unhappiest_countries, marker='o', color='red')\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.xlabel('Country')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Top 10 Unhappiest Countries by Score')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot 2: Histogram of the Score column to show its distribution\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(df['Score'], bins=20, kde=True, color='blue')\n",
        "        plt.xlabel('Happiness Score')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Distribution of Happiness Scores')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot 3: Scatter Plot between GDP per Capita and Score\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.scatterplot(x='GDP per Capita', y='Score', data=df, color='green')\n",
        "        plt.xlabel('GDP per Capita')\n",
        "        plt.ylabel('Happiness Score')\n",
        "        plt.title('Scatter Plot: GDP per Capita vs Happiness Score')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: The column '{e.args[0]}' was not found in the dataset.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing the dataset: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "id": "tnNvdauKLpK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem - 2 - Some Advance Data Exploration Task:\n",
        "Task - 1 - Setup Task - Preparing the South-Asia Dataset:"
      ],
      "metadata": {
        "id": "yElfgFLcMxuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define the list of South Asian countries\n",
        "south_asian_countries = [\n",
        "    \"Afghanistan\", \"Bangladesh\", \"Bhutan\", \"India\",\n",
        "    \"Maldives\", \"Nepal\", \"Pakistan\", \"Sri Lanka\"\n",
        "]\n",
        "\n",
        "# Step 2: Define the file path (Modify if necessary)\n",
        "file_path =  'WHR-2024-5CS037 (1).csv'  # Ensure the file path is correct\n",
        "\n",
        "# Function to attempt loading the dataset with different encodings\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            # Attempt to load the CSV file with a specific encoding\n",
        "            print(f\"Attempting to load the file with encoding: {encoding}\")\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed to load the file with encoding: {encoding}\")\n",
        "        except pd.errors.ParserError:\n",
        "            print(f\"Parsing error with encoding: {encoding}. The file may be corrupted or formatted incorrectly.\")\n",
        "            break\n",
        "    return None  # Return None if all attempts fail\n",
        "\n",
        "# Step 3: Try loading the CSV file with different encodings\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# Step 4: If the dataset was loaded successfully, filter it for South Asian countries\n",
        "if df is not None:\n",
        "    try:\n",
        "        # Step 5: Filter the dataset for South Asian countries\n",
        "        filtered_df = df[df['Country'].isin(south_asian_countries)]\n",
        "\n",
        "        # Step 6: Display the filtered DataFrame (optional check)\n",
        "        print(f\"Filtered DataFrame for South Asian countries:\\n{filtered_df.head()}\")\n",
        "\n",
        "        # Step 7: Save the filtered DataFrame as a separate CSV file\n",
        "        output_file_path = 'south_asian_happiness_scores.csv'\n",
        "        filtered_df.to_csv(output_file_path, index=False)\n",
        "        print(f\"Filtered DataFrame saved to {output_file_path}\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: The column '{e.args[0]}' was not found in the dataset.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing the dataset: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eC-y4AzM54e",
        "outputId": "46fdb055-71d5-4bb3-f3a5-369e7e9a9793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-65c8217a78c4>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-65c8217a78c4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1. Define the countries in South Asia with a list for example:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task - 2 - Composite Score Ranking:"
      ],
      "metadata": {
        "id": "J-IWfgYyOCYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the South Asian countries list (same as before)\n",
        "south_asian_countries = [\n",
        "    \"Afghanistan\", \"Bangladesh\", \"Bhutan\", \"India\",\n",
        "    \"Maldives\", \"Nepal\", \"Pakistan\", \"Sri Lanka\"\n",
        "]\n",
        "\n",
        "# Load the dataset and filter for South Asian countries\n",
        "file_path = 'WHR-2024-5CS037 (1).csv'  # Ensure the file path is correct\n",
        "\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except (UnicodeDecodeError, pd.errors.ParserError):\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "# Load dataset\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# If dataset is successfully loaded, proceed\n",
        "if df is not None:\n",
        "    # Step 3: Filter the dataset for South Asian countries\n",
        "    south_asian_df = df[df['Country'].isin(south_asian_countries)]\n",
        "\n",
        "    # Step 4: Create the Composite Score column\n",
        "    # Make sure 'GDP per Capita', 'Social Support', and 'Healthy Life Expectancy' columns exist in the dataset\n",
        "    if all(col in south_asian_df.columns for col in ['GDP per Capita', 'Social Support', 'Healthy Life Expectancy']):\n",
        "        south_asian_df['Composite Score'] = (0.40 * south_asian_df['GDP per Capita'] +\n",
        "                                              0.30 * south_asian_df['Social Support'] +\n",
        "                                              0.30 * south_asian_df['Healthy Life Expectancy'])\n",
        "\n",
        "        # Step 5: Rank the countries based on the Composite Score in descending order\n",
        "        south_asian_df = south_asian_df.sort_values(by='Composite Score', ascending=False)\n",
        "\n",
        "        # Step 6: Visualize the Top 5 countries based on Composite Score\n",
        "        top_5_composite = south_asian_df.head(5)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='Composite Score', y='Country', data=top_5_composite, palette='viridis')\n",
        "        plt.xlabel('Composite Score')\n",
        "        plt.ylabel('Country')\n",
        "        plt.title('Top 5 South Asian Countries by Composite Score')\n",
        "        plt.show()\n",
        "\n",
        "        # Step 7: Compare the Composite Score with the Original Score using a scatter plot\n",
        "        top_5_composite_scores = south_asian_df[['Country', 'Composite Score', 'Score']].head(5)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.scatterplot(x='Composite Score', y='Score', data=top_5_composite_scores, s=100, color='blue')\n",
        "        for i in range(top_5_composite_scores.shape[0]):\n",
        "            plt.text(top_5_composite_scores['Composite Score'].iloc[i],\n",
        "                     top_5_composite_scores['Score'].iloc[i],\n",
        "                     top_5_composite_scores['Country'].iloc[i],\n",
        "                     horizontalalignment='left', size='medium', color='black')\n",
        "        plt.xlabel('Composite Score')\n",
        "        plt.ylabel('Original Happiness Score')\n",
        "        plt.title('Comparison of Composite Score and Original Score for Top 5 Countries')\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Error: One or more required columns ('GDP per Capita', 'Social Support', 'Healthy Life Expectancy') are missing in the dataset.\")\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n",
        "sns.barplot(x='Composite Score', y='Country', data=top_5_composite, palette='viridis')\n",
        "sns.scatterplot(x='Composite Score', y='Score', data=top_5_composite_scores, s=100, color='blue')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fb51FKp4OFQU",
        "outputId": "0bab73af-5f45-4de0-f5a2-046b5309a0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'WHR-2024-5CS037 (1).csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b76cd7fa32cd>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_with_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# If dataset is successfully loaded, proceed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b76cd7fa32cd>\u001b[0m in \u001b[0;36mload_csv_with_encodings\u001b[0;34m(file_path, encodings)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File successfully loaded with encoding: {encoding}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WHR-2024-5CS037 (1).csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task - 3 - Outlier Detection:"
      ],
      "metadata": {
        "id": "uPnQnUmZPhZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the South Asian countries list (same as before)\n",
        "south_asian_countries = [\n",
        "    \"Afghanistan\", \"Bangladesh\", \"Bhutan\", \"India\",\n",
        "    \"Maldives\", \"Nepal\", \"Pakistan\", \"Sri Lanka\"\n",
        "]\n",
        "\n",
        "# Load the dataset and filter for South Asian countries\n",
        "file_path = 'WHR-2024-5CS037 (1).csv'  # Ensure the file path is correct\n",
        "\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except (UnicodeDecodeError, pd.errors.ParserError):\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "# Load dataset\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# If dataset is successfully loaded, proceed\n",
        "if df is not None:\n",
        "    # Step 3: Filter the dataset for South Asian countries\n",
        "    south_asian_df = df[df['Country'].isin(south_asian_countries)]\n",
        "\n",
        "    # Step 4: Calculate IQR for both 'Score' and 'GDP per Capita'\n",
        "    def identify_outliers(df, column):\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "        return outliers, lower_bound, upper_bound\n",
        "\n",
        "    # Identify outliers based on Score and GDP per Capita\n",
        "    score_outliers, score_lower, score_upper = identify_outliers(south_asian_df, 'Score')\n",
        "    gdp_outliers, gdp_lower, gdp_upper = identify_outliers(south_asian_df, 'GDP per Capita')\n",
        "\n",
        "    # Combine outliers from both Score and GDP per Capita\n",
        "    outliers_combined = pd.concat([score_outliers, gdp_outliers]).drop_duplicates()\n",
        "\n",
        "    # Step 5: Create the scatter plot with GDP per Capita on the x-axis and Score on the y-axis\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Scatter plot for all countries\n",
        "    sns.scatterplot(x='GDP per Capita', y='Score', data=south_asian_df, color='blue', label='Countries')\n",
        "\n",
        "    # Highlight outliers in red\n",
        "    sns.scatterplot(x='GDP per Capita', y='Score', data=outliers_combined, color='red', label='Outliers', s=100)\n",
        "\n",
        "    plt.title('GDP per Capita vs Happiness Score (Outliers Highlighted in Red)')\n",
        "    plt.xlabel('GDP per Capita')\n",
        "    plt.ylabel('Score (Happiness)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Step 6: Discuss characteristics of outliers and their potential impact\n",
        "    print(\"Outliers based on Score:\")\n",
        "    print(score_outliers[['Country', 'Score', 'GDP per Capita']])\n",
        "    print(\"\\nOutliers based on GDP per Capita:\")\n",
        "    print(gdp_outliers[['Country', 'Score', 'GDP per Capita']])\n",
        "\n",
        "    # Combine outliers from both Score and GDP per Capita\n",
        "    print(\"\\nCombined Outliers (Based on Score and/or GDP per Capita):\")\n",
        "    print(outliers_combined[['Country', 'Score', 'GDP per Capita']])\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "id": "idkrsSGCP5ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task - 4 - Exploring Trends Across Metrics:"
      ],
      "metadata": {
        "id": "A7E050O0QDuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Define the South Asian countries list (same as before)\n",
        "south_asian_countries = [\n",
        "    \"Afghanistan\", \"Bangladesh\", \"Bhutan\", \"India\",\n",
        "    \"Maldives\", \"Nepal\", \"Pakistan\", \"Sri Lanka\"\n",
        "]\n",
        "\n",
        "# Load the dataset and filter for South Asian countries\n",
        "file_path = 'WHR-2024-5CS037 (1).csv'  # Ensure the file path is correct\n",
        "\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except (UnicodeDecodeError, pd.errors.ParserError):\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "# Load dataset\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# If dataset is successfully loaded, proceed\n",
        "if df is not None:\n",
        "    # Step 3: Filter the dataset for South Asian countries\n",
        "    south_asian_df = df[df['Country'].isin(south_asian_countries)]\n",
        "\n",
        "    # Step 4: Calculate Pearson correlation for 'Score' with 'Freedom to Make Life Choices' and 'Generosity'\n",
        "    metrics = ['Freedom to Make Life Choices', 'Generosity']\n",
        "\n",
        "    for metric in metrics:\n",
        "        if metric in south_asian_df.columns:\n",
        "            # Calculate Pearson correlation\n",
        "            corr, _ = pearsonr(south_asian_df['Score'], south_asian_df[metric])\n",
        "            print(f\"Pearson correlation between Score and {metric}: {corr:.2f}\")\n",
        "        else:\n",
        "            print(f\"Metric '{metric}' not found in the dataset.\")\n",
        "\n",
        "    # Step 5: Create Scatter Plots with Trendlines for 'Freedom to Make Life Choices' and 'Generosity'\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i, metric in enumerate(metrics, start=1):\n",
        "        if metric in south_asian_df.columns:\n",
        "            plt.subplot(1, 2, i)\n",
        "            sns.regplot(x='Score', y=metric, data=south_asian_df, scatter_kws={'color': 'blue'}, line_kws={'color': 'red'})\n",
        "            plt.title(f'{metric} vs Score')\n",
        "            plt.xlabel('Score (Happiness)')\n",
        "            plt.ylabel(metric)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Step 6: Discuss the strongest and weakest relationships\n",
        "    # The results are printed, but we can also interpret based on the correlation coefficients printed earlier\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "id": "fBa0jlp7QFOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task - 5 - Gap Analysis:"
      ],
      "metadata": {
        "id": "PFDLZh8yQkhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the South Asian countries list\n",
        "south_asian_countries = [\n",
        "    \"Afghanistan\", \"Bangladesh\", \"Bhutan\", \"India\",\n",
        "    \"Maldives\", \"Nepal\", \"Pakistan\", \"Sri Lanka\"\n",
        "]\n",
        "\n",
        "# Load the dataset and filter for South Asian countries\n",
        "file_path = 'WHR-2024-5CS037 (1).csv'  # Ensure the file path is correct\n",
        "\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except (UnicodeDecodeError, pd.errors.ParserError):\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "# Load dataset\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# If dataset is successfully loaded, proceed\n",
        "if df is not None:\n",
        "    # Step 1: Filter the dataset for South Asian countries\n",
        "    south_asian_df = df[df['Country'].isin(south_asian_countries)]\n",
        "\n",
        "    # Step 2: Add new column for GDP-Score Gap\n",
        "    south_asian_df['GDP-Score Gap'] = south_asian_df['GDP per Capita'] - south_asian_df['Score']\n",
        "\n",
        "    # Step 3: Rank the South Asian countries by the GDP-Score Gap in ascending and descending order\n",
        "    # Sorting in descending order (largest positive gap first)\n",
        "    ascending_gap = south_asian_df[['Country', 'GDP-Score Gap']].sort_values(by='GDP-Score Gap', ascending=True)\n",
        "    descending_gap = south_asian_df[['Country', 'GDP-Score Gap']].sort_values(by='GDP-Score Gap', ascending=False)\n",
        "\n",
        "    # Display the top 3 countries with largest negative and positive gaps\n",
        "    print(\"Top 3 Countries with the Largest Negative GDP-Score Gaps:\")\n",
        "    print(ascending_gap.head(3))\n",
        "\n",
        "    print(\"\\nTop 3 Countries with the Largest Positive GDP-Score Gaps:\")\n",
        "    print(descending_gap.head(3))\n",
        "\n",
        "    # Step 4: Highlight the top 3 countries with the largest positive and negative gaps using a bar chart\n",
        "    # Plotting the bar chart for the top 3 countries with largest positive and negative gaps\n",
        "    top_3_positive = descending_gap.head(3)\n",
        "    top_3_negative = ascending_gap.head(3)\n",
        "    top_3_combined = pd.concat([top_3_positive, top_3_negative])\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='GDP-Score Gap', y='Country', data=top_3_combined, palette='coolwarm')\n",
        "    plt.title('Top 3 Countries with the Largest Positive and Negative GDP-Score Gaps')\n",
        "    plt.xlabel('GDP-Score Gap')\n",
        "    plt.ylabel('Country')\n",
        "    plt.show()\n",
        "\n",
        "    # Step 5: Analyze the reasons behind these gaps and their implications\n",
        "    print(\"\\nAnalysis of the Reasons Behind These Gaps:\")\n",
        "    print(\"The analysis will depend on the countries listed as having the largest gaps (positive and negative).\")\n",
        "    print(\"Factors that can influence these gaps include:\")\n",
        "    print(\"- High GDP per Capita but low happiness (negative gap) could indicate issues like income inequality, lack of social support, or poor health outcomes.\")\n",
        "    print(\"- High happiness despite low GDP per Capita (positive gap) could be due to factors like strong social support, good governance, or cultural aspects that prioritize well-being.\")\n",
        "else:\n",
        "    print(f\"Failed to load the dataset after multiple encoding attempts. Please check the file.\")\n"
      ],
      "metadata": {
        "id": "dlbZpBfUQorR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem - 3 - Comparative Analysis:\n",
        "Task - 1 - Setup Task - Preparing the Middle Eastern Dataset:"
      ],
      "metadata": {
        "id": "Cqjt1BdgQ_ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr, iqr\n",
        "\n",
        "# Define the lists of countries for South Asia and the Middle East\n",
        "south_asian_countries = [\n",
        "    \"Afghanistan\", \"Bangladesh\", \"Bhutan\", \"India\",\n",
        "    \"Maldives\", \"Nepal\", \"Pakistan\", \"Sri Lanka\"\n",
        "]\n",
        "\n",
        "middle_eastern_countries = [\n",
        "    \"Bahrain\", \"Iran\", \"Iraq\", \"Israel\", \"Jordan\",\n",
        "    \"Kuwait\", \"Lebanon\", \"Oman\", \"Palestine\", \"Qatar\",\n",
        "    \"Saudi Arabia\", \"Syria\", \"United Arab Emirates\", \"Yemen\"\n",
        "]\n",
        "\n",
        "# Load the dataset (modify the path if necessary)\n",
        "file_path = 'WHR-2024-5CS037 (1).csv'  # Ensure the file path is correct\n",
        "\n",
        "def load_csv_with_encodings(file_path, encodings=['utf-8', 'ISO-8859-1', 'latin1', 'utf-16']):\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"File successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except (UnicodeDecodeError, pd.errors.ParserError):\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "# Load the dataset\n",
        "df = load_csv_with_encodings(file_path)\n",
        "\n",
        "# Filter the dataset for South Asian and Middle Eastern countries\n",
        "if df is not None:\n",
        "    south_asian_df = df[df['Country'].isin(south_asian_countries)]\n",
        "    middle_eastern_df = df[df['Country'].isin(middle_eastern_countries)]\n",
        "\n",
        "    # 1. Descriptive Statistics: Calculate mean and standard deviation for Score\n",
        "    south_asian_mean_score = south_asian_df['Score'].mean()\n",
        "    south_asian_std_score = south_asian_df['Score'].std()\n",
        "\n",
        "    middle_eastern_mean_score = middle_eastern_df['Score'].mean()\n",
        "    middle_eastern_std_score = middle_eastern_df['Score'].std()\n",
        "\n",
        "    print(f\"South Asia - Mean Score: {south_asian_mean_score:.2f}, Std Dev: {south_asian_std_score:.2f}\")\n",
        "    print(f\"Middle East - Mean Score: {middle_eastern_mean_score:.2f}, Std Dev: {middle_eastern_std_score:.2f}\")\n",
        "\n",
        "    # Determine which region has higher average happiness scores\n",
        "    higher_avg_score_region = \"South Asia\" if south_asian_mean_score > middle_eastern_mean_score else \"Middle East\"\n",
        "    print(f\"\\nRegion with higher average happiness score: {higher_avg_score_region}\")\n",
        "\n",
        "    # 2. Top and Bottom Performers\n",
        "    top_3_south_asia = south_asian_df[['Country', 'Score']].sort_values(by='Score', ascending=False).head(3)\n",
        "    bottom_3_south_asia = south_asian_df[['Country', 'Score']].sort_values(by='Score', ascending=True).head(3)\n",
        "\n",
        "    top_3_middle_east = middle_eastern_df[['Country', 'Score']].sort_values(by='Score', ascending=False).head(3)\n",
        "    bottom_3_middle_east = middle_eastern_df[['Country', 'Score']].sort_values(by='Score', ascending=True).head(3)\n",
        "\n",
        "    # Plot bar charts for top 3 and bottom 3 performers in both regions\n",
        "    plt.figure(figsize=(14, 7))\n",
        "\n",
        "    # Top Performers\n",
        "    plt.subplot(1, 2, 1)\n",
        "    top_3_combined = pd.concat([top_3_south_asia, top_3_middle_east])\n",
        "    sns.barplot(x='Score', y='Country', data=top_3_combined, palette='coolwarm')\n",
        "    plt.title('Top 3 Happiest Countries')\n",
        "    plt.xlabel('Happiness Score')\n",
        "    plt.ylabel('Country')\n",
        "\n",
        "    # Bottom Performers\n",
        "    plt.subplot(1, 2, 2)\n",
        "    bottom_3_combined = pd.concat([bottom_3_south_asia, bottom_3_middle_east])\n",
        "    sns.barplot(x='Score', y='Country', data=bottom_3_combined, palette='coolwarm')\n",
        "    plt.title('Bottom 3 Happiest Countries')\n",
        "    plt.xlabel('Happiness Score')\n",
        "    plt.ylabel('Country')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Metric Comparisons: Compare GDP per Capita, Social Support, and Healthy Life Expectancy\n",
        "    metrics = ['GDP per Capita', 'Social Support', 'Healthy Life Expectancy']\n",
        "\n",
        "    # Create grouped bar charts comparing metrics between regions\n",
        "    south_asian_metrics = south_asian_df[['Country'] + metrics].mean()\n",
        "    middle_eastern_metrics = middle_eastern_df[['Country'] + metrics].mean()\n",
        "\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'South Asia': south_asian_metrics,\n",
        "        'Middle East': middle_eastern_metrics\n",
        "    }).drop('Country')\n",
        "\n",
        "    comparison_df.plot(kind='bar', figsize=(10, 6), color=['blue', 'orange'])\n",
        "    plt.title('Comparison of Key Metrics between South Asia and the Middle East')\n",
        "    plt.ylabel('Mean Value')\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Happiness Disparity: Compute Range and Coefficient of Variation (CV)\n",
        "    def calculate_range_and_cv(df, region):\n",
        "        happiness_range = df['Score'].max() - df['Score'].min()\n",
        "        cv = df['Score'].std() / df['Score'].mean()\n",
        "        print(f\"\\n{region} - Range of Score: {happiness_range:.2f}, Coefficient of Variation: {cv:.2f}\")\n",
        "        return happiness_range, cv\n",
        "\n",
        "    south_asian_range, south_asian_cv = calculate_range_and_cv(south_asian_df, \"South Asia\")\n",
        "    middle_eastern_range, middle_eastern_cv = calculate_range_and_cv(middle_eastern_df, \"Middle East\")\n",
        "\n",
        "    # 5. Correlation Analysis: Correlation of Score with Freedom to Make Life Choices, Generosity\n",
        "    def correlation_analysis(df, region):\n",
        "        corr_freedom = pearsonr(df['Score'], df['Freedom to Make Life Choices'])[0]\n",
        "        corr_generosity = pearsonr(df['Score'], df['Generosity'])[0]\n",
        "        print(f\"\\n{region} - Correlation between Score and Freedom to Make Life Choices: {corr_freedom:.2f}\")\n",
        "        print(f\"{region} - Correlation between Score and Generosity: {corr_generosity:.2f}\")\n",
        "\n",
        "        # Scatter plots with trendlines\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Scatter plot for Freedom to Make Life Choices\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.regplot(x='Freedom to Make Life Choices', y='Score', data=df, scatter_kws={'color': 'blue'}, line_kws={'color': 'red'})\n",
        "        plt.title(f'{region}: Freedom to Make Life Choices vs Score')\n",
        "        plt.xlabel('Freedom to Make Life Choices')\n",
        "        plt.ylabel('Happiness Score')\n",
        "\n",
        "        # Scatter plot for Generosity\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.regplot(x='Generosity', y='Score', data=df, scatter_kws={'color': 'green'}, line_kws={'color': 'red'})\n",
        "        plt.title(f'{region}: Generosity vs Score')\n",
        "        plt.xlabel('Generosity')\n",
        "        plt.ylabel('Happiness Score')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    correlation_analysis(south_asian_df, \"South Asia\")\n",
        "    correlation_analysis(middle_eastern_df, \"Middle East\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nBjRYQ5LRAl3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "1435442f-5c5d-4195-ec42-4299f500a81b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'WHR-2024-5CS037 (1).csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-032d7722e0e7>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_with_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Filter the dataset for South Asian and Middle Eastern countries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-032d7722e0e7>\u001b[0m in \u001b[0;36mload_csv_with_encodings\u001b[0;34m(file_path, encodings)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File successfully loaded with encoding: {encoding}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WHR-2024-5CS037 (1).csv'"
          ]
        }
      ]
    }
  ]
}